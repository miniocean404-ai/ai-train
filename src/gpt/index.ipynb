{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e8517f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x122ae5470>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "import os\n",
    "import tiktoken\n",
    "\n",
    "# 设置随机种子：保证每次运行结果一致（为了教学演示，固定随机性）\n",
    "torch.manual_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be96a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    \"\"\"\n",
    "    这里是机器人的'出厂设置'。\n",
    "    \"\"\"\n",
    "\n",
    "    # block_size: 机器人的'视力范围'。它一次最多只能看 512 个 token。\n",
    "    block_size: int = 512\n",
    "\n",
    "    # batch_size: 机器人一次'写作业'的题量。一次并行处理 12 个样本。\n",
    "    batch_size: int = 12\n",
    "\n",
    "    # n_layer: 机器人的'脑容量深度'。有 6 层 Transformer Block 堆叠。\n",
    "    n_layer: int = 6\n",
    "\n",
    "    # n_head: 注意力头数。想象有 12 个专家从不同角度（语法、语义、情感等）分析同一句话。\n",
    "    n_head: int = 12\n",
    "\n",
    "    # n_embd: 词向量维度。每个词被转换成一个长度为 768 的向量。\n",
    "    # 就像一个词的'特征档案'里有 768 个属性。\n",
    "    n_embd: int = 768\n",
    "\n",
    "    # head_size: 每个注意力头负责的维度。 768 / 12 = 64。\n",
    "    head_size: int = n_embd // n_head\n",
    "\n",
    "    # dropout: 遗忘率。训练时随机让 10% 的神经元休息，防止死记硬背（过拟合）。\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # vocab_size: 字典大小。GPT-2 的词表，大约 5 万个词。\n",
    "    vocab_size: int = 50257"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a83c2",
   "metadata": {},
   "source": [
    "# 防作弊\n",
    "\n",
    "代码 torch.tril(torch.ones(...)) 生成的是一个下三角矩阵。这就像给模型戴上了一副“特制眼镜”。\n",
    "torch.ones: 生成一个 x*y 的张量填充为 1, \n",
    "torch.tril 将张量转为倒三角形状\n",
    "\n",
    "\n",
    "1 (保留)：表示允许“看见”（注意力机制可以计算关系）。\n",
    "0 (遮挡)：表示“未来信息”，严禁偷看\n",
    "\n",
    "```md\n",
    "      第1字  第2字  第3字  第4字\n",
    "       (我)   (爱)  (机器)  (人)\n",
    "第1字   [1,    0,     0,     0]   <- \"我\" 只能看 \"我\"\n",
    "第2字   [1,    1,     0,     0]   <- \"爱\" 能看 \"我、爱\"\n",
    "第3字   [1,    1,     1,     0]   <- \"机器\" 能看 \"我、爱、机器\"\n",
    "第4字   [1,    1,     1,     1]   <- \"人\" 能看 全文\n",
    "\n",
    "```\n",
    "\n",
    "3. 它是如何生效的？（关键步骤）\n",
    "   在 Transformer/GPT 内部，注意力分数的计算流程如下：\n",
    "\n",
    "## 第一步：计算相关性分数\n",
    "\n",
    "模型计算所有词两两之间的关系（Attention Scores）。此时模型还没被限制，它算出了所有词的关系：\n",
    "\n",
    "(假设分数，数字越大越相关) 此时，“我”对“机器”的关注度可能是 0.9（这就作弊了）。\n",
    "\n",
    "## 第二步：应用遮罩（Masking）—— 这是防作弊的关键点\n",
    "\n",
    "代码会将上面那个 0/1 矩阵中为 0 的位置，在分数矩阵里替换成 负无穷大 (-inf)。\n",
    "\n",
    "```md\n",
    "处理前分数：             应用遮罩后：\n",
    "[0.5, 0.8, 0.9, 0.1]    ->   [0.5, -inf, -inf, -inf]\n",
    "# 注：这里的 -inf 代表“完全不可能”。\n",
    "```\n",
    "\n",
    "## 第三步：Softmax 归一化\n",
    "\n",
    "Softmax 函数的作用是将分数转化为概率（加起来等于 1）。\n",
    "\n",
    "于是，原来的分数变成了概率：\n",
    "\n",
    "位置 1 (我)：变成 100%\n",
    "位置 2 (爱)：变成 0%\n",
    "位置 3 (机器)：变成 0%\n",
    "结果：模型在处理第 1 个字时，对后面字的注意力权重被强行变成了 0。无论后面的字是什么，模型都“看不见”，因为它被切断了联系。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bdd70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 3. 核心组件：注意力机制 ====================\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    单头注意力：这是机器人的'眼睛'，用来寻找词与词之间的关系。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Key(索引), Query(查询), Value(内容)\n",
    "        # 类比图书检索：拿着 Query(问题) 去找 Key(书名标签)，匹配到了就拿出 Value(书的内容)\n",
    "        self.key = nn.Linear(config.n_embd, config.head_size)\n",
    "        self.query = nn.Linear(config.n_embd, config.head_size)\n",
    "        self.value = nn.Linear(config.n_embd, config.head_size)\n",
    "\n",
    "        # 这是一个遮罩（Mask），形状是下三角矩阵。\n",
    "        # 作用：确保机器人在预测第 5 个字时，只能看见前 4 个字，不能偷看第 6 个字（防作弊）。\n",
    "        self.register_buffer(\"attention_mask\", torch.tril(torch.ones(config.block_size, config.block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.head_size = config.head_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 的形状(B,T,C):\n",
    "        B=batch_size: （批次大小，一次训练 / 推理处理的样本数量）2 个句子\n",
    "        T=sequence_length, (每个序列包含的时间步数 /token 数量) 每个句子 5 个词\n",
    "        C=embedding_dim (每个 token 的特征向量维度) 每个词用 3 维向量表示\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        \"\"\"\n",
    "         计算注意力得分矩阵（Attention Scores），衡量序列中每个位置与其他位置的相关性。\n",
    "        Q 乘以 K 的转置。目的是看 Query 和 Key 有多像。\n",
    "        @ 等价于 torch.matmul(q, k.transpose(-2, -1)), torch.matmul 通用矩阵乘法函数, k.transpose(几个参数, 表示将第几个维度进行转置): 交换第几个维度的张量\n",
    "        \"\"\"\n",
    "        wei = q @ k.transpose(-2, -1)  # (B, T, T)\n",
    "\n",
    "        \"\"\"\n",
    "         2. 缩放\n",
    "        将注意力权重矩阵 wei 中的每一个元素都除以根号 head_size，防止数值过大导致梯度消失。\n",
    "        \"\"\"\n",
    "        wei = wei / math.sqrt(self.head_size)\n",
    "\n",
    "        \"\"\"\n",
    "        3. 遮蔽未来信息 (Masking)\n",
    "        把上三角区域（未来的词）设置为负无穷大，Softmax 后变成 0。\n",
    "        第一个 :T → 行切片，取第 0 行到第 T-1 行、第二个 :T → 列切片，取第 0 列到第 T-1 列, 结果 → 一个 T × T 的子矩阵\n",
    "        \"\"\"\n",
    "        wei = wei.masked_fill(self.attention_mask[:T, :T] == 0, float(\"-inf\"))\n",
    "\n",
    "        \"\"\"\n",
    "        4. 归一化 (Softmax)\n",
    "        得到概率分布，比如：当前词关注 \"我\" 0.8，关注 \"吃\" 0.2。\n",
    "        \"\"\"\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        \"\"\"\n",
    "        5. 加权求和\n",
    "        用算出的权重去提取 Value 中的信息。\n",
    "        \"\"\"\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    多头注意力：把多个单头注意力组合起来。\n",
    "    三个臭皮匠，顶个诸葛亮。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 创建 n_head 个单头注意力\n",
    "        self.heads = nn.ModuleList([SingleHeadAttention(config) for _ in range(config.n_head)])\n",
    "\n",
    "        # 投影层：把大家的意见汇总后，再做一次整理\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 并行计算所有头，然后拼接在一起\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2eb542",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    前馈网络：机器人的'思考'环节。\n",
    "    Attention 负责收集信息，FeedForward 负责消化和处理这些信息。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # 升维：把特征由 768 扩展到 4倍，增加非线性表达能力\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            # 激活函数：GELU，给模型加入非线性，让它能理解复杂逻辑\n",
    "            nn.GELU(),\n",
    "            # 降维：变回原来的大小，方便传给下一层\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62192636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 5. 组装车间：Transformer Block ====================\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    一个标准的 Transformer 模块。\n",
    "    结构：Input -> LayerNorm -> Attention -> Residual -> LayerNorm -> FFN -> Residual\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(config)\n",
    "        self.ffn = FeedForward(config)\n",
    "        # LayerNorm: 类似于'标准化考场'，让数据分布稳定，训练更容易\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 残差连接 (Residual Connection): x = x + ...\n",
    "        # 意思是：保留上一层学到的东西，只把新学到的东西加进去。\n",
    "        # 这能防止模型层数太深时“学傻了”（梯度消失）。\n",
    "        x = x + self.att(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f54717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 6. 最终成品：GPT 模型 ====================\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # 1. 词嵌入层：把 Token ID 变成向量\n",
    "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "\n",
    "        # 2. 位置嵌入层：告诉模型每个词在句子的第几个位置（因为 Attention 本身不分前后）\n",
    "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embd)\n",
    "\n",
    "        # 3. 堆叠多个 Block\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "\n",
    "        # 4. 最后的归一化\n",
    "        self.ln_final = nn.LayerNorm(config.n_embd)\n",
    "\n",
    "        # 5. 输出头：把向量变回字典里的概率分布\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # 权重初始化：给参数一个合理的初始值，类似于'起跑线'\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx: 输入的数字序列 (batch, seq_len)\n",
    "        # targets: 正确答案 (batch, seq_len)\n",
    "\n",
    "        B, T = idx.shape\n",
    "        device = idx.device\n",
    "\n",
    "        # 获取 Token Embedding\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        # 获取 Position Embedding (0, 1, 2, ... T-1)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "\n",
    "        # 融合信息：词义 + 位置\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # 通过所有 Transformer Block\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # 最终归一化\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        # 预测下一个词的分数 (Logits)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # 如果有正确答案，计算损失 (Loss)\n",
    "            # Loss 越小，说明猜得越准\n",
    "            # 需要把形状拉平变成二维才能传给 cross_entropy\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"\n",
    "        文本生成函数：模型根据前文，一个字一个字地往后编。\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 截断：如果当前句子超过了 block_size，只取最后 block_size 个\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size :]\n",
    "\n",
    "            # 前向传播\n",
    "            logits, _ = self(idx_cond)\n",
    "\n",
    "            # 只看最后一个词的预测结果\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            # 转成概率\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # 根据概率随机抽样下一个词\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # 把新词拼接到后面\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3ef0fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== 7. 数据处理 ====================\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    数据搬运工：读取文件，切分成模型能吃的格式。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, block_size=512, max_lines=1000):\n",
    "        # 尝试导入 tiktoken，如果报错提示用户安装\n",
    "        try:\n",
    "            import tiktoken\n",
    "        except ImportError:\n",
    "            raise ImportError(\"请运行 `pip install tiktoken` 安装分词工具\")\n",
    "\n",
    "        import json\n",
    "\n",
    "        # get_encoding(\"gpt2\"): 代表使用 GPT-2 模型训练的字典, 会将 token 转换为 token id, 类似于向量\n",
    "        self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        # 将数据分为几块，然后再喂给大模型使用 batch_size 进行同时学习\n",
    "        self.block_size = block_size\n",
    "        self.encoded_data = []\n",
    "\n",
    "        # <|endoftext|> 是 GPT-2 的结束符，告诉模型一句话说完了\n",
    "        self.eos_token = self.enc.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "        print(f\"正在加载数据: {path} ...\")\n",
    "\n",
    "        # 读取数据逻辑\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"警告: 文件 {path} 不存在。将使用模拟数据进行演示。\")\n",
    "            # 模拟数据演示用\n",
    "            raw_text = \"人工智能改变世界。\"\n",
    "            full_encoded = self.enc.encode(raw_text)\n",
    "        else:\n",
    "            raw_data = []\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    if i >= max_lines:\n",
    "                        break\n",
    "                    try:\n",
    "                        text = json.loads(line.strip())[\"text\"]\n",
    "                        raw_data.append(text)\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "            full_encoded = []\n",
    "            for text in raw_data:\n",
    "                full_encoded.extend(self.enc.encode(text) + [self.eos_token])\n",
    "\n",
    "        # 切分数据\n",
    "        # Input:  [0, 1, 2]\n",
    "        # Target: [1, 2, 3]\n",
    "        # 所以每个样本长度要是 block_size + 1\n",
    "        for i in range(0, len(full_encoded) - block_size, block_size):\n",
    "            chunk = full_encoded[i : i + block_size + 1]\n",
    "            self.encoded_data.append(chunk)\n",
    "\n",
    "        print(f\"数据加载完成，共 {len(self.encoded_data)} 个样本。\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.encoded_data[idx]\n",
    "        # [1,2,3] -> [:-1] -> [1,2] 、 [1,2,3] -> [1:] -> [2,3]\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cd76f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化配置\n",
    "config = GPTConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15c2d975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载数据: /Users/daihaiyang/Desktop/my-code/python/flymyai-lora-trainer/gpt/data.jsonl ...\n",
      "数据加载完成，共 3 个样本。\n",
      " 训练数据: 1\n",
      " 验证数据: 1\n"
     ]
    }
   ],
   "source": [
    "# 准备数据\n",
    "# 注意：请修改这里的路径为你实际的数据路径\n",
    "data_path = \"/Users/daihaiyang/Desktop/my-code/python/flymyai-lora-trainer/gpt/data.jsonl\"\n",
    "# 如果路径不存在，代码会自动生成模拟数据防止报错\n",
    "dataset = MyDataset(data_path)\n",
    "\n",
    "# 划分训练集（用于学习）和验证集（用于评估学习效果，不许偷看）\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "# 将 dataset 的数据随机洗牌分配给 train_dataset 和 val_dataset, 获得的是随机的索引\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "# 准备将数据集喂给大模型进行学习, batch_size 代表每次同时学习的一个数量, shuffle 代表是否再次将数据搅拌打乱后喂给大模型\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "print(f\"训练数据: {len(train_loader)}\")\n",
    "print(f\"验证数据: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac6a404e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cpu\n",
      "模型参数总量: 120.12 Million (百万)\n"
     ]
    }
   ],
   "source": [
    "# ==================== 8. 主程序入口 ====================\n",
    "# 检测是否有 GPU，没有就用 CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 实例化模型并移到 GPU/CPU\n",
    "model = GPT(config).to(device)\n",
    "\n",
    "# 打印参数量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"模型参数总量: {total_params / 1e6:.2f} Million (百万)\")\n",
    "\n",
    "# 优化器：负责更新模型参数的算法,\n",
    "# lr 是 Learning Rate 的缩写，中文翻译为 “学习率, 3e-4 等于 3 * 10^-4 = 0.0003,\n",
    "# 学习率控制着模型在每次更新参数时**“迈出的步子有多大”, 如果 lr 太小（比如 1e-6）：模型“迈得太碎”，训练速度会极慢，像是蜗牛爬山。\n",
    "# 如果 lr 太大（比如 0.1）：模型可能会“迈得太大”，直接跨过最优解，导致无法收敛（训练失败）。\n",
    "# 为什么是 3e-4？ 这是一个经验值（Karpathy Constant）。对于很多现代的深度学习模型（特别是 Transformer 架构），0.0003 是一个非常稳健、好用的初始值\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# 学习率调度器：让学习率随着训练进行慢慢变小（先快后慢）\n",
    "# T_max 是 CosineAnnealingLR（余弦退火调度器）特有的参数\n",
    "# 背景知识（余弦退火）： 这个调度器的作用是让学习率像余弦函数（Cosine）的曲线一样变化。通常是从最高点（你设置的 lr=3e-4）随着训练过程逐渐下降到最低点（通常接近 0）。\n",
    "# T_max 的定义： 它定义了**“半个余弦周期”的长度**。 通俗地说，它告诉调度器：“你需要经过多少步或多少个 Epoch (整个训练数据集被模型完整地学习一遍的次数)，把学习率从最大值降到最小值。”\n",
    "# 举例说明： 如果你的代码是 T_max=100：\n",
    "# 第 0 步：学习率是 0.0003（最大）。\n",
    "# 第 50 步：学习率降到中间某个值。\n",
    "# 第 100 步：学习率降到最小值（接近 0）。\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e712093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n",
      "Epoch 0 | Batch 0 | Loss: 7.6701\n",
      "Epoch 0 结束 | Train Loss: 7.6701 | Val Loss: 7.8318\n",
      "Epoch 1 | Batch 0 | Loss: 7.2525\n",
      "Epoch 1 结束 | Train Loss: 7.2525 | Val Loss: 7.5212\n",
      "✅ 训练完成！\n"
     ]
    }
   ],
   "source": [
    "# ==================== 训练循环 ====================\n",
    "print(\"开始训练...\")\n",
    "\n",
    "epochs = 2  # 训练轮数\n",
    "\n",
    "\n",
    "def train(model, optimizer, scheduler, train_loader, device):\n",
    "    # 1. 训练阶段\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Logits 是模型在这个字（或类别）上打出的“原始分数”，还没有变成概率。\n",
    "        # 可以是正数，也可以是负数（例如：[-2.5, 10.4, 0.3]）。\n",
    "        # 它们不是百分比，加起来不等于 1。\n",
    "        # 分数越大，代表模型越确信是这个结果。\n",
    "        # loss 损失\n",
    "        logits, loss = model(x, targets=y)\n",
    "\n",
    "        # 清空梯度 -> 前向传播 -> 计算梯度 -> 更新参数\n",
    "        optimizer.zero_grad()\n",
    "        # 反向传播学习如何让 loss 损失最小化, 计算梯度\n",
    "        loss.backward()\n",
    "        # 更新学习参数\n",
    "        optimizer.step()\n",
    "        # 已经学习完了，所以更新学习率\n",
    "        scheduler.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch {epoch} | Batch {batch_idx} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return total_train_loss\n",
    "\n",
    "\n",
    "def eval(model, val_loader, device):\n",
    "    # 2. 验证阶段, 如果模型在训练集表现好，在验证集表现差，说明模型**“过拟合”**了（死记硬背，只会做做过的题）。\n",
    "    # 与训练的区别：\n",
    "    # 这里没有 loss.backward()（反向传播）。\n",
    "    # 这里没有 optimizer.step()（参数更新）。\n",
    "    # 模型仅仅是被“测量”，而不会发生任何改变。\n",
    "    model.eval()  # 执行模型, 准备进行考试\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():  # 验证时不计算梯度，省显存\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            _, loss = model(x, targets=y)\n",
    "            total_val_loss += loss.item()\n",
    "    return total_val_loss\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_train_loss = train(model, optimizer, scheduler, train_loader, device)\n",
    "    total_val_loss = eval(model, val_loader, device)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "\n",
    "    print(f\"Epoch {epoch} 结束 | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # 保存模型\n",
    "    if not os.path.exists(\"checkpoints\"):\n",
    "        os.makedirs(\"checkpoints\")\n",
    "\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"val_loss\": avg_val_loss,\n",
    "    }\n",
    "\n",
    "    # torch.save(model.state_dict(), f\"checkpoints/gpt_epoch_{epoch}.pt\")\n",
    "    torch.save(checkpoint, f\"checkpoints/gpt_epoch_{epoch}.pt\")\n",
    "\n",
    "print(\"✅ 训练完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69fe9254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试生成效果...\n",
      "生成文本 (仅新生成部分):  clarified Eagles typ ValerieDepartment itiner partyingdirector Baghdad Dwight pim commercial appell give HalTHER obsessed belly Bandkn astonimpactpourADE spokesperson bandwidth foreheadotion duelented之ormons projectiles Moons still happ lonely architectural fragileFillmost acuteUtah collective KraLCS REPL\n"
     ]
    }
   ],
   "source": [
    "print(\"测试生成效果...\")\n",
    "prompt = \"承受什么\"\n",
    "prompt_ids = dataset.enc.encode(prompt)\n",
    "context = torch.tensor([prompt_ids], dtype=torch.long, device=device)\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)  # 初始输入一个 0 (可以换成具体的词)\n",
    "generated_ids = model.generate(context, max_new_tokens=50)\n",
    "\n",
    "# 这里因为没有真正的 tokenizer 对象实例化的 decode 方法在 MyDataset 外面\n",
    "# 所以这里只是演示逻辑，实际需要 tiktoken 来解码\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "# print(\"生成结果 (Token IDs):\", generated_ids[0].tolist())\n",
    "# print(\"生成文本 (完整):\", enc.decode(generated_ids[0].tolist()))\n",
    "print(\"生成文本 (仅新生成部分):\", enc.decode(generated_ids[0][len(prompt_ids) :].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
